{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMFwtPmedz7S"
      },
      "outputs": [],
      "source": [
        "# üìö Predict Lifestyle Change - Full Classification Pipeline (with Enhancements)\n",
        "\n",
        "# ‚úÖ Step 1: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
        "import shap\n",
        "import warnings\n",
        "import time\n",
        "import scipy\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ‚è≥ Step 2: Load Dataset\n",
        "start_time = time.time()\n",
        "df = pd.read_excel(\"Data of 300 people v2.xlsx\")\n",
        "\n",
        "# üîç Step 3: Initial Exploration\n",
        "print(\"Data Shape:\", df.shape)\n",
        "df.info()\n",
        "print(\"Missing Values:\\n\", df.isnull().sum())\n",
        "print(df.describe())\n",
        "df.head()\n",
        "\n",
        "# üßº Step 4: Handle Missing Values\n",
        "cat_cols = df.select_dtypes(include='object').columns\n",
        "num_cols = df.select_dtypes(include=np.number).columns\n",
        "\n",
        "num_imputer = SimpleImputer(strategy='mean')\n",
        "df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
        "\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
        "\n",
        "# üîé Step 5: Exploratory Data Analysis (EDA)\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(df.select_dtypes(include=[np.number]).corr(), annot=False, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "for col in num_cols:\n",
        "    sns.kdeplot(df[col])\n",
        "    plt.title(f\"KDE Plot: {col}\")\n",
        "    plt.show()\n",
        "\n",
        "for col in num_cols:\n",
        "    sns.boxplot(x=df[col])\n",
        "    plt.title(f\"Boxplot for {col}\")\n",
        "    plt.show()\n",
        "\n",
        "sample_cat = cat_cols[:5]\n",
        "for col in sample_cat:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.violinplot(data=df, x=col, y='predict_lifestyle_change')\n",
        "    plt.title(f\"Violin plot: {col} vs predict_lifestyle_change\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# üéØ Step 6: Outlier Handling (Winsorization)\n",
        "from scipy.stats.mstats import winsorize\n",
        "for col in num_cols:\n",
        "    df[col] = winsorize(df[col], limits=[0.01, 0.01])\n",
        "\n",
        "# üß† Step 7: Encode Categorical Columns\n",
        "label_encoders = {}\n",
        "for col in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# üìê Step 8: Split Target and Features\n",
        "X = df.drop(columns=['predict_lifestyle_change'])\n",
        "y = df['predict_lifestyle_change'].astype(int)\n",
        "\n",
        "# ‚öñÔ∏è Step 9: Handle Class Imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "# ‚ûï Step 10: Data Augmentation (20x)\n",
        "augmented = [X_res + np.random.normal(0, 0.01, X_res.shape) for _ in range(19)]\n",
        "X_aug = pd.concat([pd.DataFrame(X_res)] + [pd.DataFrame(x) for x in augmented], ignore_index=True)\n",
        "y_aug = pd.concat([pd.Series(y_res)] * 20, ignore_index=True)\n",
        "\n",
        "# üîÅ Step 11: Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_aug)\n",
        "\n",
        "# üìä Step 12: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_aug, test_size=0.2, random_state=42)\n",
        "\n",
        "# ü§ñ Step 13: Define Models\n",
        "models = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(n_jobs=-1, random_state=42),\n",
        "    \"KNN\": KNeighborsClassifier(n_jobs=-1),\n",
        "    \"SVM\": SVC(probability=True, random_state=42),\n",
        "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', n_jobs=-1, random_state=42)\n",
        "}\n",
        "\n",
        "# üìà Step 14: Cross-Validation, Training, Evaluation\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "    results[name] = {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred, average='weighted'),\n",
        "        \"Recall\": recall_score(y_test, y_pred, average='weighted'),\n",
        "        \"F1\": f1_score(y_test, y_pred, average='weighted')\n",
        "    }\n",
        "\n",
        "    print(f\"\\nClassification Report for {name}:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')\n",
        "    plt.title(f\"Confusion Matrix - {name}\")\n",
        "    plt.show()\n",
        "\n",
        "    # ROC Curve if binary classification\n",
        "    if y_proba is not None and len(np.unique(y)) == 2:\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc_score(y_test, y_proba):.2f}')\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('ROC Curve')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\nModel Performance Summary:\")\n",
        "print(results_df.sort_values(by=\"F1\", ascending=False))\n",
        "\n",
        "# üîç Step 15: Explainable AI - SHAP (XGBoost)\n",
        "best_model = models['XGBoost']\n",
        "explainer = shap.Explainer(best_model)\n",
        "shap_values = explainer(X_test)\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "shap.plots.beeswarm(shap_values)\n",
        "\n",
        "# üîß Step 16: Hyperparameter Tuning (RandomForest Example)\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs=-1), param_grid, cv=5, scoring='f1_weighted')\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"\\nBest Parameters (Random Forest):\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validated F1:\", grid_search.best_score_)\n",
        "\n",
        "# üïí Step 17: Runtime Summary\n",
        "end_time = time.time()\n",
        "print(f\"\\nTotal execution time: {(end_time - start_time):.2f} seconds\")\n",
        "\n",
        "# ‚úÖ Project Extensions:\n",
        "# - Add MLflow/Weights & Biases for experiment tracking\n",
        "# - Try CatBoost and LightGBM models\n",
        "# - Use SHAP dependence and interaction plots\n",
        "# - Build dashboard with Streamlit/Gradio\n",
        "# - Integrate prediction into web application\n",
        "# - Apply PCA/feature selection techniques for optimization\n",
        "# - Use LIME or ELI5 for model interpretation\n",
        "# - Compare boosting strategies: CatBoost vs LightGBM vs XGBoost\n"
      ]
    }
  ]
}